{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77759cf0-b884-4fe5-8ec9-802cbdfd3c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LogicModule...\n",
      "Gaze model 'gaze_lock_model_finetuned.keras' loaded successfully.\n",
      "Object detection model 'yolov8n.pt' loaded successfully.\n",
      "LogicModule initialized.\n",
      "Starting monitoring...\n",
      "System will activate object detection when sustained away gaze is detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:24, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:24, INFO, Object detector deactivated (Gaze back on screen)\n",
      "2025-07-26 14:45:30, INFO, Face detected, Gaze confidence: 0.28\n",
      "2025-07-26 14:45:30, INFO, Gaze changed: Away, Confidence: 0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:45:30] ObjectDetector: ACTIVATED!\n",
      "\n",
      "0: 480x640 1 cat, 189.6ms\n",
      "Speed: 13.1ms preprocess, 189.6ms inference, 17.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:30, INFO, Sustained away gaze detected, activating object detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 153.0ms\n",
      "Speed: 2.1ms preprocess, 153.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 143.9ms\n",
      "Speed: 2.4ms preprocess, 143.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 154.6ms\n",
      "Speed: 2.5ms preprocess, 154.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 cats, 129.8ms\n",
      "Speed: 2.5ms preprocess, 129.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 cats, 136.9ms\n",
      "Speed: 2.1ms preprocess, 136.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 127.4ms\n",
      "Speed: 2.7ms preprocess, 127.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 143.5ms\n",
      "Speed: 2.0ms preprocess, 143.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 170.8ms\n",
      "Speed: 5.0ms preprocess, 170.8ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 140.6ms\n",
      "Speed: 5.2ms preprocess, 140.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 cats, 117.4ms\n",
      "Speed: 2.4ms preprocess, 117.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 130.1ms\n",
      "Speed: 2.3ms preprocess, 130.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 cat, 125.6ms\n",
      "Speed: 4.5ms preprocess, 125.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:35, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:35, INFO, Object detector deactivated (Gaze back on screen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:45:35] ObjectDetector: DEACTIVATED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:35, INFO, Face detected, Gaze confidence: 0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:45:37] ObjectDetector: ACTIVATED!\n",
      "\n",
      "0: 480x640 1 cat, 136.6ms\n",
      "Speed: 2.6ms preprocess, 136.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:37, INFO, Sustained away gaze detected, activating object detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 120.6ms\n",
      "Speed: 2.4ms preprocess, 120.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 116.9ms\n",
      "Speed: 2.2ms preprocess, 116.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:38, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:38, INFO, Object detector deactivated (Gaze back on screen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:45:38] ObjectDetector: DEACTIVATED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:39, INFO, Face detected, Gaze confidence: 0.00\n",
      "2025-07-26 14:45:40, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:40, INFO, Face detected, Gaze confidence: 0.33\n",
      "2025-07-26 14:45:41, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:42, INFO, Face detected, Gaze confidence: 0.22\n",
      "2025-07-26 14:45:42, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:42, INFO, Face detected, Gaze confidence: 0.13\n",
      "2025-07-26 14:45:42, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:44, INFO, Face detected, Gaze confidence: 0.01\n",
      "2025-07-26 14:45:44, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:44, INFO, Face detected, Gaze confidence: 0.01\n",
      "2025-07-26 14:45:44, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:45, INFO, Face detected, Gaze confidence: 0.02\n",
      "2025-07-26 14:45:45, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:45, INFO, Face detected, Gaze confidence: 0.17\n",
      "2025-07-26 14:45:45, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:47, INFO, Face detected, Gaze confidence: 0.28\n",
      "2025-07-26 14:45:47, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:47, INFO, Face detected, Gaze confidence: 0.01\n",
      "2025-07-26 14:45:48, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:48, INFO, Face detected, Gaze confidence: 0.01\n",
      "2025-07-26 14:45:48, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:48, INFO, Face detected, Gaze confidence: 0.06\n",
      "2025-07-26 14:45:49, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:49, INFO, Face detected, Gaze confidence: 0.05\n",
      "2025-07-26 14:45:49, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:50, INFO, Face detected, Gaze confidence: 0.00\n",
      "2025-07-26 14:45:50, INFO, Face lost / not detected\n",
      "2025-07-26 14:45:50, INFO, Face detected, Gaze confidence: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:45:52] ObjectDetector: ACTIVATED!\n",
      "\n",
      "0: 480x640 1 person, 228.2ms\n",
      "Speed: 2.2ms preprocess, 228.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:53, INFO, Sustained away gaze detected, activating object detection\n",
      "2025-07-26 14:45:53, INFO, Detected object: person (0.85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 181.4ms\n",
      "Speed: 4.3ms preprocess, 181.4ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:53, INFO, Detected object: person (0.85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 165.1ms\n",
      "Speed: 2.8ms preprocess, 165.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:54, INFO, Detected object: person (0.82)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 167.7ms\n",
      "Speed: 2.9ms preprocess, 167.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:54, INFO, Detected object: person (0.88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 181.5ms\n",
      "Speed: 3.1ms preprocess, 181.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:54, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 230.5ms\n",
      "Speed: 3.2ms preprocess, 230.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:55, INFO, Detected object: person (0.83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 239.1ms\n",
      "Speed: 6.1ms preprocess, 239.1ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:56, INFO, Detected object: person (0.86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 240.0ms\n",
      "Speed: 3.7ms preprocess, 240.0ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:56, INFO, Detected object: person (0.88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 178.4ms\n",
      "Speed: 3.1ms preprocess, 178.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:57, INFO, Detected object: person (0.84)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 221.5ms\n",
      "Speed: 7.1ms preprocess, 221.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:57, INFO, Detected object: person (0.81)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 162.8ms\n",
      "Speed: 3.2ms preprocess, 162.8ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:58, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 160.4ms\n",
      "Speed: 2.5ms preprocess, 160.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:58, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 155.3ms\n",
      "Speed: 2.9ms preprocess, 155.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:58, INFO, Detected object: person (0.90)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 167.8ms\n",
      "Speed: 2.3ms preprocess, 167.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:59, INFO, Detected object: person (0.86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 184.0ms\n",
      "Speed: 2.6ms preprocess, 184.0ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:45:59, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 sports ball, 164.2ms\n",
      "Speed: 2.4ms preprocess, 164.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:00, INFO, Detected object: person (0.87)\n",
      "2025-07-26 14:46:00, INFO, Detected object: person (0.32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 157.3ms\n",
      "Speed: 8.2ms preprocess, 157.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:00, INFO, Detected object: person (0.75)\n",
      "2025-07-26 14:46:00, INFO, Detected object: person (0.36)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 remote, 160.9ms\n",
      "Speed: 2.4ms preprocess, 160.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:01, INFO, Detected object: person (0.81)\n",
      "2025-07-26 14:46:01, INFO, Detected object: person (0.59)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 2 remotes, 161.6ms\n",
      "Speed: 3.0ms preprocess, 161.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:01, INFO, Detected object: person (0.69)\n",
      "2025-07-26 14:46:01, INFO, Detected object: person (0.28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 remote, 1 cell phone, 158.3ms\n",
      "Speed: 2.9ms preprocess, 158.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:01, INFO, Detected object: person (0.79)\n",
      "2025-07-26 14:46:01, INFO, Detected object: person (0.68)\n",
      "2025-07-26 14:46:01, INFO, Detected object: cell phone (0.30)\n",
      "2025-07-26 14:46:01, INFO, Face lost / not detected\n",
      "2025-07-26 14:46:01, INFO, Object detector deactivated (Gaze back on screen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:01] ObjectDetector: DEACTIVATED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:02, INFO, Face detected, Gaze confidence: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:04] ObjectDetector: ACTIVATED!\n",
      "\n",
      "0: 480x640 1 person, 229.1ms\n",
      "Speed: 3.0ms preprocess, 229.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:04, INFO, Sustained away gaze detected, activating object detection\n",
      "2025-07-26 14:46:04, INFO, Detected object: person (0.86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 223.4ms\n",
      "Speed: 4.1ms preprocess, 223.4ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:05, INFO, Detected object: person (0.83)\n",
      "2025-07-26 14:46:05, INFO, Detected object: person (0.50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 210.3ms\n",
      "Speed: 6.4ms preprocess, 210.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:05, INFO, Detected object: person (0.81)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 224.6ms\n",
      "Speed: 3.1ms preprocess, 224.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:06, INFO, Detected object: person (0.76)\n",
      "2025-07-26 14:46:06, INFO, Detected object: person (0.51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 228.6ms\n",
      "Speed: 3.5ms preprocess, 228.6ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:06, INFO, Detected object: person (0.76)\n",
      "2025-07-26 14:46:06, INFO, Detected object: cell phone (0.44)\n",
      "2025-07-26 14:46:06, INFO, Detected object: person (0.34)\n",
      "2025-07-26 14:46:06, INFO, Detected object: person (0.26)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 178.2ms\n",
      "Speed: 2.7ms preprocess, 178.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:07, INFO, Detected object: person (0.66)\n",
      "2025-07-26 14:46:07, INFO, Detected object: person (0.52)\n",
      "2025-07-26 14:46:07, INFO, Detected object: cell phone (0.49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 223.5ms\n",
      "Speed: 3.2ms preprocess, 223.5ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:07, INFO, Detected object: person (0.71)\n",
      "2025-07-26 14:46:07, INFO, Detected object: person (0.51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 188.2ms\n",
      "Speed: 2.8ms preprocess, 188.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:08, INFO, Detected object: person (0.61)\n",
      "2025-07-26 14:46:08, INFO, Detected object: person (0.44)\n",
      "2025-07-26 14:46:08, INFO, Detected object: cell phone (0.29)\n",
      "2025-07-26 14:46:08, INFO, Detected object: person (0.25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 164.2ms\n",
      "Speed: 2.5ms preprocess, 164.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:08, INFO, Detected object: person (0.72)\n",
      "2025-07-26 14:46:08, INFO, Detected object: person (0.48)\n",
      "2025-07-26 14:46:08, INFO, Detected object: cell phone (0.34)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 163.3ms\n",
      "Speed: 2.5ms preprocess, 163.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:09, INFO, Detected object: person (0.66)\n",
      "2025-07-26 14:46:09, INFO, Detected object: person (0.58)\n",
      "2025-07-26 14:46:09, INFO, Detected object: cell phone (0.55)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 158.5ms\n",
      "Speed: 2.3ms preprocess, 158.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:09, INFO, Detected object: person (0.78)\n",
      "2025-07-26 14:46:09, INFO, Detected object: person (0.25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 mouse, 160.3ms\n",
      "Speed: 4.2ms preprocess, 160.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:10, INFO, Detected object: person (0.88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 4 persons, 1 frisbee, 160.7ms\n",
      "Speed: 2.2ms preprocess, 160.7ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:10, INFO, Detected object: person (0.60)\n",
      "2025-07-26 14:46:10, INFO, Detected object: person (0.53)\n",
      "2025-07-26 14:46:10, INFO, Detected object: person (0.41)\n",
      "2025-07-26 14:46:10, INFO, Detected object: person (0.37)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 166.1ms\n",
      "Speed: 3.2ms preprocess, 166.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:10, INFO, Detected object: person (0.91)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 206.4ms\n",
      "Speed: 4.6ms preprocess, 206.4ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:11, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 158.2ms\n",
      "Speed: 3.0ms preprocess, 158.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:11, INFO, Detected object: person (0.92)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 175.1ms\n",
      "Speed: 2.4ms preprocess, 175.1ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:12, INFO, Detected object: person (0.91)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 162.2ms\n",
      "Speed: 2.4ms preprocess, 162.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:12, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 184.0ms\n",
      "Speed: 2.3ms preprocess, 184.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:13, INFO, Detected object: person (0.76)\n",
      "2025-07-26 14:46:13, INFO, Detected object: person (0.56)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 162.8ms\n",
      "Speed: 2.1ms preprocess, 162.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:13, INFO, Detected object: person (0.93)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 161.0ms\n",
      "Speed: 3.1ms preprocess, 161.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:13, INFO, Detected object: person (0.83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 179.1ms\n",
      "Speed: 3.5ms preprocess, 179.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:14, INFO, Detected object: person (0.94)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 177.1ms\n",
      "Speed: 3.8ms preprocess, 177.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:14, INFO, Detected object: person (0.83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 230.2ms\n",
      "Speed: 6.1ms preprocess, 230.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:15, INFO, Detected object: person (0.91)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 232.2ms\n",
      "Speed: 4.2ms preprocess, 232.2ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:15, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 215.3ms\n",
      "Speed: 2.8ms preprocess, 215.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:16, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 220.1ms\n",
      "Speed: 5.5ms preprocess, 220.1ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:16, INFO, Detected object: person (0.91)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 234.2ms\n",
      "Speed: 3.2ms preprocess, 234.2ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:17, INFO, Detected object: person (0.75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 222.1ms\n",
      "Speed: 5.5ms preprocess, 222.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:17, INFO, Detected object: person (0.86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 183.1ms\n",
      "Speed: 2.7ms preprocess, 183.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:18, INFO, Detected object: person (0.84)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 212.0ms\n",
      "Speed: 2.2ms preprocess, 212.0ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:18, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 218.2ms\n",
      "Speed: 3.9ms preprocess, 218.2ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:19, INFO, Detected object: person (0.86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 161.9ms\n",
      "Speed: 2.3ms preprocess, 161.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:19, INFO, Detected object: person (0.86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 217.3ms\n",
      "Speed: 3.0ms preprocess, 217.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:20, INFO, Detected object: person (0.68)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 185.5ms\n",
      "Speed: 3.0ms preprocess, 185.5ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:20, INFO, Detected object: person (0.62)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 186.5ms\n",
      "Speed: 2.9ms preprocess, 186.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:21, INFO, Detected object: person (0.71)\n",
      "2025-07-26 14:46:21, INFO, Detected object: person (0.42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 177.3ms\n",
      "Speed: 2.6ms preprocess, 177.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:21, INFO, Detected object: person (0.54)\n",
      "2025-07-26 14:46:21, INFO, Detected object: person (0.50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 188.0ms\n",
      "Speed: 2.8ms preprocess, 188.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:22, INFO, Detected object: person (0.85)\n",
      "2025-07-26 14:46:22, INFO, Face lost / not detected\n",
      "2025-07-26 14:46:22, INFO, Object detector deactivated (Gaze back on screen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:22] ObjectDetector: DEACTIVATED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:22, INFO, Face detected, Gaze confidence: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:24] ObjectDetector: ACTIVATED!\n",
      "\n",
      "0: 480x640 1 person, 203.5ms\n",
      "Speed: 2.8ms preprocess, 203.5ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:24, INFO, Sustained away gaze detected, activating object detection\n",
      "2025-07-26 14:46:24, INFO, Detected object: person (0.74)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 171.6ms\n",
      "Speed: 3.3ms preprocess, 171.6ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:25, INFO, Detected object: person (0.84)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 166.9ms\n",
      "Speed: 2.1ms preprocess, 166.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:25, INFO, Detected object: person (0.83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 toothbrush, 182.1ms\n",
      "Speed: 2.1ms preprocess, 182.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:26, INFO, Detected object: person (0.85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 158.9ms\n",
      "Speed: 3.5ms preprocess, 158.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:26, INFO, Detected object: person (0.78)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 158.4ms\n",
      "Speed: 3.4ms preprocess, 158.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:26, INFO, Detected object: person (0.83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 190.7ms\n",
      "Speed: 3.2ms preprocess, 190.7ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:27, INFO, Detected object: person (0.73)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 207.7ms\n",
      "Speed: 2.1ms preprocess, 207.7ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:27, INFO, Detected object: person (0.78)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 211.3ms\n",
      "Speed: 3.4ms preprocess, 211.3ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:28, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 202.9ms\n",
      "Speed: 3.5ms preprocess, 202.9ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:28, INFO, Detected object: person (0.83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 214.4ms\n",
      "Speed: 5.6ms preprocess, 214.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:29, INFO, Detected object: person (0.81)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 210.6ms\n",
      "Speed: 3.4ms preprocess, 210.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:30, INFO, Detected object: person (0.73)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 158.3ms\n",
      "Speed: 4.0ms preprocess, 158.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:30, INFO, Detected object: person (0.77)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 160.8ms\n",
      "Speed: 6.8ms preprocess, 160.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:30, INFO, Detected object: person (0.68)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 157.1ms\n",
      "Speed: 3.2ms preprocess, 157.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:31, INFO, Detected object: person (0.81)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 161.9ms\n",
      "Speed: 2.5ms preprocess, 161.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:31, INFO, Detected object: person (0.83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 176.0ms\n",
      "Speed: 3.0ms preprocess, 176.0ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:32, INFO, Detected object: person (0.75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 166.5ms\n",
      "Speed: 2.9ms preprocess, 166.5ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:32, INFO, Detected object: person (0.86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 202.5ms\n",
      "Speed: 6.2ms preprocess, 202.5ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:32, INFO, Detected object: person (0.69)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 169.8ms\n",
      "Speed: 2.7ms preprocess, 169.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:33, INFO, Detected object: person (0.68)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 155.1ms\n",
      "Speed: 3.8ms preprocess, 155.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:33, INFO, Detected object: person (0.65)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 201.7ms\n",
      "Speed: 3.8ms preprocess, 201.7ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:34, INFO, Detected object: person (0.84)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 206.0ms\n",
      "Speed: 5.6ms preprocess, 206.0ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:34, INFO, Detected object: person (0.76)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 200.6ms\n",
      "Speed: 2.4ms preprocess, 200.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:35, INFO, Detected object: person (0.77)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 156.0ms\n",
      "Speed: 2.7ms preprocess, 156.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:35, INFO, Detected object: person (0.78)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 153.3ms\n",
      "Speed: 2.3ms preprocess, 153.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:36, INFO, Detected object: person (0.85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 162.5ms\n",
      "Speed: 2.3ms preprocess, 162.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:36, INFO, Detected object: person (0.78)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 159.4ms\n",
      "Speed: 2.1ms preprocess, 159.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:36, INFO, Detected object: person (0.69)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 170.7ms\n",
      "Speed: 2.2ms preprocess, 170.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:37, INFO, Detected object: person (0.79)\n",
      "2025-07-26 14:46:37, INFO, Detected object: person (0.36)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 162.5ms\n",
      "Speed: 4.4ms preprocess, 162.5ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:37, INFO, Detected object: person (0.57)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:37] ObjectDetector: DEACTIVATED."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:37, INFO, Face lost / not detected\n",
      "2025-07-26 14:46:37, INFO, Object detector deactivated (Gaze back on screen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:38, INFO, Face detected, Gaze confidence: 0.00\n",
      "2025-07-26 14:46:39, INFO, Face lost / not detected\n",
      "2025-07-26 14:46:40, INFO, Face detected, Gaze confidence: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:42] ObjectDetector: ACTIVATED!\n",
      "\n",
      "0: 480x640 2 persons, 1 tennis racket, 1 cell phone, 170.2ms\n",
      "Speed: 2.4ms preprocess, 170.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:43, INFO, Sustained away gaze detected, activating object detection\n",
      "2025-07-26 14:46:43, INFO, Detected object: person (0.67)\n",
      "2025-07-26 14:46:43, INFO, Detected object: person (0.45)\n",
      "2025-07-26 14:46:43, INFO, Detected object: cell phone (0.28)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 159.4ms\n",
      "Speed: 2.0ms preprocess, 159.4ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:43, INFO, Detected object: person (0.70)\n",
      "2025-07-26 14:46:43, INFO, Detected object: person (0.61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 156.2ms\n",
      "Speed: 2.3ms preprocess, 156.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:43, INFO, Detected object: person (0.70)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 157.8ms\n",
      "Speed: 2.7ms preprocess, 157.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:44, INFO, Detected object: person (0.80)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 156.8ms\n",
      "Speed: 2.0ms preprocess, 156.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:44, INFO, Detected object: person (0.70)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 184.9ms\n",
      "Speed: 4.1ms preprocess, 184.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:45, INFO, Detected object: person (0.84)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 214.3ms\n",
      "Speed: 2.6ms preprocess, 214.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:45, INFO, Detected object: person (0.71)\n",
      "2025-07-26 14:46:45, INFO, Detected object: person (0.49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 204.5ms\n",
      "Speed: 3.1ms preprocess, 204.5ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:46, INFO, Detected object: person (0.71)\n",
      "2025-07-26 14:46:46, INFO, Detected object: person (0.66)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 220.4ms\n",
      "Speed: 4.2ms preprocess, 220.4ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:46, INFO, Detected object: person (0.84)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 226.2ms\n",
      "Speed: 4.5ms preprocess, 226.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:47, INFO, Detected object: person (0.85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 224.9ms\n",
      "Speed: 5.9ms preprocess, 224.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:47, INFO, Detected object: person (0.83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 183.6ms\n",
      "Speed: 2.7ms preprocess, 183.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:48, INFO, Detected object: person (0.85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 178.1ms\n",
      "Speed: 4.1ms preprocess, 178.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:48, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 182.6ms\n",
      "Speed: 2.4ms preprocess, 182.6ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:49, INFO, Detected object: person (0.88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 200.7ms\n",
      "Speed: 2.9ms preprocess, 200.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:49, INFO, Detected object: person (0.88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 201.3ms\n",
      "Speed: 3.2ms preprocess, 201.3ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:50, INFO, Detected object: person (0.86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 156.7ms\n",
      "Speed: 2.7ms preprocess, 156.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:50, INFO, Detected object: person (0.75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 166.7ms\n",
      "Speed: 4.7ms preprocess, 166.7ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:50, INFO, Detected object: person (0.71)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 175.3ms\n",
      "Speed: 3.3ms preprocess, 175.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:51, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 205.7ms\n",
      "Speed: 4.2ms preprocess, 205.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:51, INFO, Detected object: person (0.85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 196.8ms\n",
      "Speed: 5.0ms preprocess, 196.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:52, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 200.3ms\n",
      "Speed: 3.5ms preprocess, 200.3ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:52, INFO, Detected object: person (0.88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 193.6ms\n",
      "Speed: 3.5ms preprocess, 193.6ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:53, INFO, Detected object: person (0.91)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 189.7ms\n",
      "Speed: 3.0ms preprocess, 189.7ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:53, INFO, Detected object: person (0.88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 185.7ms\n",
      "Speed: 3.4ms preprocess, 185.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:54, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 216.2ms\n",
      "Speed: 4.8ms preprocess, 216.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:54, INFO, Detected object: person (0.90)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 185.0ms\n",
      "Speed: 5.2ms preprocess, 185.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:55, INFO, Detected object: person (0.90)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 167.5ms\n",
      "Speed: 2.4ms preprocess, 167.5ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:55, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 198.8ms\n",
      "Speed: 3.1ms preprocess, 198.8ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:56, INFO, Detected object: person (0.87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 182.6ms\n",
      "Speed: 2.8ms preprocess, 182.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:56, INFO, Detected object: person (0.90)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 166.3ms\n",
      "Speed: 2.6ms preprocess, 166.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:57, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 202.9ms\n",
      "Speed: 2.9ms preprocess, 202.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:57, INFO, Detected object: person (0.90)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 238.9ms\n",
      "Speed: 6.7ms preprocess, 238.9ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:58, INFO, Detected object: person (0.89)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 240.4ms\n",
      "Speed: 4.6ms preprocess, 240.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:58, INFO, Detected object: person (0.61)\n",
      "2025-07-26 14:46:58, INFO, Detected object: person (0.41)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 229.6ms\n",
      "Speed: 4.3ms preprocess, 229.6ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:46:59, INFO, Detected object: person (0.78)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 254.8ms\n",
      "Speed: 6.2ms preprocess, 254.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:00, INFO, Detected object: person (0.84)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 241.6ms\n",
      "Speed: 3.7ms preprocess, 241.6ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:00, INFO, Detected object: person (0.79)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 243.3ms\n",
      "Speed: 6.1ms preprocess, 243.3ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:01, INFO, Detected object: person (0.63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 242.9ms\n",
      "Speed: 5.9ms preprocess, 242.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:01, INFO, Detected object: person (0.71)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 236.8ms\n",
      "Speed: 5.0ms preprocess, 236.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:02, INFO, Detected object: person (0.71)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 190.7ms\n",
      "Speed: 3.6ms preprocess, 190.7ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:02, INFO, Detected object: person (0.64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 190.1ms\n",
      "Speed: 2.4ms preprocess, 190.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:03, INFO, Detected object: person (0.80)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 267.8ms\n",
      "Speed: 6.1ms preprocess, 267.8ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:03, INFO, Detected object: person (0.80)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 238.0ms\n",
      "Speed: 4.1ms preprocess, 238.0ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:04, INFO, Detected object: person (0.80)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 215.7ms\n",
      "Speed: 2.8ms preprocess, 215.7ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:05, INFO, Detected object: person (0.74)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 191.4ms\n",
      "Speed: 4.4ms preprocess, 191.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:05, INFO, Detected object: person (0.77)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 193.0ms\n",
      "Speed: 2.4ms preprocess, 193.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:47:06, INFO, Detected object: person (0.88)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program interrupted by user (Ctrl+C).\n",
      "Stopping monitoring and releasing resources...\n",
      "GazeDetector resources released.\n",
      "Monitoring stopped and resources released.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import logging\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Logger helper ---\n",
    "class StateLogger:\n",
    "    def __init__(self, log_txt_path=\"log.txt\", log_csv_path=\"log.csv\"):\n",
    "        self.log_txt_path = log_txt_path\n",
    "        self.log_csv_path = log_csv_path\n",
    "\n",
    "        self.logger = logging.getLogger(\"StateLogger\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s, %(levelname)s, %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Console\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setFormatter(formatter)\n",
    "        self.logger.addHandler(ch)\n",
    "\n",
    "        # Plain text file\n",
    "        fh = logging.FileHandler(log_txt_path)\n",
    "        fh.setFormatter(formatter)\n",
    "        self.logger.addHandler(fh)\n",
    "\n",
    "        # Create CSV header if file doesn't exist\n",
    "        try:\n",
    "            with open(self.log_csv_path, mode='x', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(['timestamp', 'level', 'message'])\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "    def log_event(self, message, level='INFO'):\n",
    "        # Log to console + txt\n",
    "        if level == 'INFO':\n",
    "            self.logger.info(message)\n",
    "        elif level == 'WARNING':\n",
    "            self.logger.warning(message)\n",
    "        elif level == 'ERROR':\n",
    "            self.logger.error(message)\n",
    "        else:\n",
    "            self.logger.info(message)\n",
    "\n",
    "        # Also write to CSV\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        with open(self.log_csv_path, mode='a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([timestamp, level, message])\n",
    "\n",
    "# --- Constants ---\n",
    "GAZE_MODEL_PATH = \"gaze_lock_model_finetuned.keras\"\n",
    "OBJECT_MODEL_PATH = 'yolov8n.pt'\n",
    "IMPORTANT_CLASSES = ['person', 'cell phone', 'laptop', 'book', 'tv']\n",
    "\n",
    "# Eye landmark indices\n",
    "LEFT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]\n",
    "RIGHT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]\n",
    "\n",
    "class GazeDetector:\n",
    "    def __init__(self, model_path=GAZE_MODEL_PATH, buffer_size=5):\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "            print(f\"Gaze model '{model_path}' loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading gaze model from '{model_path}': {e}\")\n",
    "            # Create a dummy model if loading fails\n",
    "            input_left = tf.keras.Input(shape=(64, 64, 3), name=\"left_eye_input\")\n",
    "            input_right = tf.keras.Input(shape=(64, 64, 3), name=\"right_eye_input\")\n",
    "            dummy_output = tf.keras.layers.Dense(2, activation='softmax')(tf.keras.layers.Flatten()(input_left))\n",
    "            self.model = tf.keras.Model(inputs=[input_left, input_right], outputs=dummy_output)\n",
    "            print(\"Using a dummy gaze model as a fallback.\")\n",
    "\n",
    "        # MediaPipe initialization\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        self.LEFT_EYE_INDICES = LEFT_EYE_INDICES\n",
    "        self.RIGHT_EYE_INDICES = RIGHT_EYE_INDICES\n",
    "        self.confidence_buffer = deque(maxlen=buffer_size)\n",
    "        self.internal_gaze_threshold = 0.5\n",
    "\n",
    "    def _calculate_eye_rect(self, eye_landmarks, image_width, image_height):\n",
    "        x_coords = [lm.x * image_width for lm in eye_landmarks]\n",
    "        y_coords = [lm.y * image_height for lm in eye_landmarks]\n",
    "\n",
    "        if not x_coords or not y_coords:\n",
    "            return (0,0,0,0)\n",
    "\n",
    "        eye_width = max(x_coords) - min(x_coords)\n",
    "        margin = int(eye_width * 0.5)\n",
    "        x_min = int(min(x_coords) - margin)\n",
    "        x_max = int(max(x_coords) + margin)\n",
    "        y_min = int(min(y_coords) - margin)\n",
    "        y_max = int(max(y_coords) + margin)\n",
    "\n",
    "        return (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "    def _extract_and_preprocess_eye(self, image, rect):\n",
    "        x, y, w, h = rect\n",
    "        if w <= 0 or h <= 0:\n",
    "            return None\n",
    "\n",
    "        img_h, img_w = image.shape[:2]\n",
    "        x_start = max(0, x)\n",
    "        y_start = max(0, y)\n",
    "        x_end = min(img_w, x + w)\n",
    "        y_end = min(img_h, y + h)\n",
    "\n",
    "        if x_end <= x_start or y_end <= y_start:\n",
    "            return None\n",
    "\n",
    "        eye_img = image[y_start:y_end, x_start:x_end]\n",
    "        if eye_img.size == 0:\n",
    "            return None\n",
    "\n",
    "        eye_gray = cv2.cvtColor(eye_img, cv2.COLOR_BGR2GRAY)\n",
    "        eye_gray = cv2.equalizeHist(eye_gray)\n",
    "        eye_img_processed = cv2.cvtColor(eye_gray, cv2.COLOR_GRAY2BGR)\n",
    "        eye_img_processed = cv2.resize(eye_img_processed, (64, 64))\n",
    "        eye_img_processed = cv2.cvtColor(eye_img_processed, cv2.COLOR_BGR2RGB)\n",
    "        eye_img_processed = eye_img_processed.astype(np.float32) / 255.0\n",
    "\n",
    "        return eye_img_processed\n",
    "\n",
    "    def _predict_gaze(self, left_eye_img, right_eye_img):\n",
    "        left_eye_batch = np.expand_dims(left_eye_img, axis=0)\n",
    "        right_eye_batch = np.expand_dims(right_eye_img, axis=0)\n",
    "        prediction = self.model.predict([left_eye_batch, right_eye_batch], verbose=0)\n",
    "        confidence_looking_at_screen = prediction[0][1] \n",
    "        return confidence_looking_at_screen\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        original_frame = frame.copy()\n",
    "        h, w, _ = frame.shape\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(image_rgb)\n",
    "\n",
    "        gaze_info = {\n",
    "            \"raw_confidence\": 0.0,\n",
    "            \"smoothed_confidence\": 0.0,\n",
    "            \"is_looking\": False,\n",
    "            \"face_detected\": False,\n",
    "            \"left_eye_img_processed\": None,\n",
    "            \"right_eye_img_processed\": None,\n",
    "            \"annotated_frame\": original_frame\n",
    "        }\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            gaze_info[\"face_detected\"] = True\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "            left_eye_landmarks = [face_landmarks.landmark[i] for i in self.LEFT_EYE_INDICES]\n",
    "            right_eye_landmarks = [face_landmarks.landmark[i] for i in self.RIGHT_EYE_INDICES]\n",
    "\n",
    "            left_rect = self._calculate_eye_rect(left_eye_landmarks, w, h)\n",
    "            right_rect = self._calculate_eye_rect(right_eye_landmarks, w, h)\n",
    "\n",
    "            left_eye_img_processed = self._extract_and_preprocess_eye(original_frame, left_rect)\n",
    "            right_eye_img_processed = self._extract_and_preprocess_eye(original_frame, right_rect)\n",
    "            \n",
    "            gaze_info[\"left_eye_img_processed\"] = left_eye_img_processed\n",
    "            gaze_info[\"right_eye_img_processed\"] = right_eye_img_processed\n",
    "\n",
    "            if left_eye_img_processed is not None and right_eye_img_processed is not None:\n",
    "                raw_confidence = self._predict_gaze(left_eye_img_processed, right_eye_img_processed)\n",
    "                gaze_info[\"raw_confidence\"] = float(raw_confidence)\n",
    "                self.confidence_buffer.append(raw_confidence)\n",
    "                if self.confidence_buffer:\n",
    "                     gaze_info[\"smoothed_confidence\"] = float(sum(self.confidence_buffer) / len(self.confidence_buffer))\n",
    "                \n",
    "                gaze_info[\"is_looking\"] = gaze_info[\"smoothed_confidence\"] > self.internal_gaze_threshold\n",
    "\n",
    "                status_text = f\"Raw: {gaze_info['raw_confidence']:.2%} Smooth: {gaze_info['smoothed_confidence']:.2%}\"\n",
    "                gaze_status_text = \"Looking\" if gaze_info[\"is_looking\"] else \"Away\"\n",
    "                color = (0, 255, 0) if gaze_info[\"is_looking\"] else (0, 0, 255)\n",
    "\n",
    "                cv2.putText(gaze_info[\"annotated_frame\"], status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (230, 230, 230), 2)\n",
    "                cv2.putText(gaze_info[\"annotated_frame\"], gaze_status_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "                cv2.rectangle(gaze_info[\"annotated_frame\"], (left_rect[0], left_rect[1]),\n",
    "                              (left_rect[0] + left_rect[2], left_rect[1] + left_rect[3]), (0, 255, 0), 1)\n",
    "                cv2.rectangle(gaze_info[\"annotated_frame\"], (right_rect[0], right_rect[1]),\n",
    "                              (right_rect[0] + right_rect[2], right_rect[1] + right_rect[3]), (0, 255, 0), 1)\n",
    "                \n",
    "                if gaze_info[\"left_eye_img_processed\"] is not None:\n",
    "                    debug_left_eye = (gaze_info[\"left_eye_img_processed\"] * 255).astype(np.uint8)\n",
    "                    debug_left_eye = cv2.resize(debug_left_eye, (96, 96))\n",
    "                    y_offset_eye = 10\n",
    "                    x_offset_left_eye = gaze_info[\"annotated_frame\"].shape[1] - 96*2 - 20\n",
    "                    if x_offset_left_eye > 0 and y_offset_eye + 96 < gaze_info[\"annotated_frame\"].shape[0]:\n",
    "                         gaze_info[\"annotated_frame\"][y_offset_eye:y_offset_eye+96, x_offset_left_eye:x_offset_left_eye+96] = debug_left_eye\n",
    "\n",
    "                if gaze_info[\"right_eye_img_processed\"] is not None:\n",
    "                    debug_right_eye = (gaze_info[\"right_eye_img_processed\"] * 255).astype(np.uint8)\n",
    "                    debug_right_eye = cv2.resize(debug_right_eye, (96, 96))\n",
    "                    x_offset_right_eye = gaze_info[\"annotated_frame\"].shape[1] - 96 - 10\n",
    "                    if x_offset_right_eye > 0 and y_offset_eye + 96 < gaze_info[\"annotated_frame\"].shape[0]:\n",
    "                        gaze_info[\"annotated_frame\"][y_offset_eye:y_offset_eye+96, x_offset_right_eye:x_offset_right_eye+96] = debug_right_eye\n",
    "            else:\n",
    "                 cv2.putText(gaze_info[\"annotated_frame\"], \"Eye crop failed\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "        else:\n",
    "            cv2.putText(gaze_info[\"annotated_frame\"], \"No face detected\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            self.confidence_buffer.clear()\n",
    "\n",
    "        return gaze_info\n",
    "\n",
    "    # def release(self):\n",
    "    #     if hasattr(self, 'face_mesh') and self.face_mesh:\n",
    "    #         self.face_mesh.close()\n",
    "    #     print(\"GazeDetector resources released.\")\n",
    "    def release(self):\n",
    "        if hasattr(self, 'face_mesh') and self.face_mesh:\n",
    "            if hasattr(self.face_mesh, '_graph') and self.face_mesh._graph is not None:\n",
    "                self.face_mesh.close()\n",
    "        print(\"GazeDetector resources released.\")\n",
    "\n",
    "\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self, model_path=OBJECT_MODEL_PATH, important_classes=IMPORTANT_CLASSES):\n",
    "        try:\n",
    "            self.model = YOLO(model_path)\n",
    "            print(f\"Object detection model '{model_path}' loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading object detection model: {e}\")\n",
    "            self.model = None\n",
    "        \n",
    "        self.important_classes = important_classes\n",
    "        self.is_active = False\n",
    "        self.last_activation_time = 0\n",
    "        self.activation_cooldown = 2  # seconds\n",
    "    \n",
    "    def detect_objects(self, frame):\n",
    "        if not self.is_active or self.model is None:\n",
    "            return []\n",
    "        \n",
    "        results = self.model(frame)[0]\n",
    "        important_objects = []\n",
    "        \n",
    "        for box in results.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            cls_name = results.names[cls_id]\n",
    "            if cls_name in self.important_classes:\n",
    "                conf = float(box.conf[0])\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                important_objects.append({\n",
    "                    'class_name': cls_name,\n",
    "                    'confidence': conf,\n",
    "                    'box': (x1, y1, x2, y2)\n",
    "                })\n",
    "        \n",
    "        return important_objects\n",
    "\n",
    "    def activate(self):\n",
    "        current_time = time.time()\n",
    "        if not self.is_active and (current_time - self.last_activation_time > self.activation_cooldown):\n",
    "            self.is_active = True\n",
    "            self.last_activation_time = current_time\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] ObjectDetector: ACTIVATED!\")\n",
    "            return True\n",
    "        elif self.is_active:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] ObjectDetector: Already active.\")\n",
    "            return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def deactivate(self):\n",
    "        if self.is_active:\n",
    "            self.is_active = False\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] ObjectDetector: DEACTIVATED.\")\n",
    "\n",
    "\n",
    "class LogicModule:\n",
    "    def __init__(self, \n",
    "                 gaze_model_path=GAZE_MODEL_PATH, \n",
    "                 sustained_away_threshold=2.0,  # seconds of sustained away gaze to trigger object detection\n",
    "                 show_video_feed=True):\n",
    "        print(\"Initializing LogicModule...\")\n",
    "        self.gaze_detector = GazeDetector(model_path=gaze_model_path)\n",
    "        self.object_detector = ObjectDetector()\n",
    "        self.sustained_away_threshold = sustained_away_threshold\n",
    "        self.show_video_feed = show_video_feed\n",
    "        self.cap = None\n",
    "        self.running = False\n",
    "\n",
    "        # Logger and previous states\n",
    "        self.logger = StateLogger()\n",
    "        self.prev_face_detected = None\n",
    "        self.prev_gaze_status = None\n",
    "        self.prev_od_status = None\n",
    "        \n",
    "        # For tracking sustained away gaze\n",
    "        self.away_start_time = None\n",
    "        self.away_duration = 0.0\n",
    "        \n",
    "        print(\"LogicModule initialized.\")\n",
    "\n",
    "    def start_monitoring(self):\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        if not self.cap.isOpened():\n",
    "            print(\"Error: Could not open video camera.\")\n",
    "            return\n",
    "\n",
    "        print(\"Starting monitoring...\")\n",
    "        print(\"System will activate object detection when sustained away gaze is detected.\")\n",
    "        self.running = True\n",
    "        \n",
    "        while self.running:\n",
    "            success, frame = self.cap.read()\n",
    "            if not success:\n",
    "                print(\"Failed to grab frame or end of video stream.\")\n",
    "                break\n",
    "\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Process gaze detection\n",
    "            gaze_results = self.gaze_detector.process_frame(frame)\n",
    "            \n",
    "            # Update sustained away gaze tracking\n",
    "            if gaze_results[\"face_detected\"]:\n",
    "                if not gaze_results[\"is_looking\"]:\n",
    "                    # Gaze is away\n",
    "                    if self.away_start_time is None:\n",
    "                        self.away_start_time = current_time\n",
    "                    else:\n",
    "                        self.away_duration = current_time - self.away_start_time\n",
    "                else:\n",
    "                    # Gaze is back on screen\n",
    "                    self.away_start_time = None\n",
    "                    self.away_duration = 0.0\n",
    "                    if self.object_detector.is_active:\n",
    "                        self.object_detector.deactivate()\n",
    "            else:\n",
    "                # No face detected\n",
    "                self.away_start_time = None\n",
    "                self.away_duration = 0.0\n",
    "                if self.object_detector.is_active:\n",
    "                    self.object_detector.deactivate()\n",
    "            \n",
    "            # Activate object detector if sustained away gaze threshold is reached\n",
    "            if (self.away_duration >= self.sustained_away_threshold and \n",
    "                not self.object_detector.is_active):\n",
    "                self.object_detector.activate()\n",
    "            \n",
    "            # Run object detection if active\n",
    "            if self.object_detector.is_active:\n",
    "                important_objects = self.object_detector.detect_objects(frame)\n",
    "            else:\n",
    "                important_objects = []\n",
    "\n",
    "            # --- Log on state changes ---\n",
    "            face_now = gaze_results[\"face_detected\"]\n",
    "            if face_now != self.prev_face_detected:\n",
    "                if face_now:\n",
    "                    self.logger.log_event(f\"Face detected, Gaze confidence: {gaze_results['smoothed_confidence']:.2f}\")\n",
    "                else:\n",
    "                    self.logger.log_event(\"Face lost / not detected\")\n",
    "                self.prev_face_detected = face_now\n",
    "\n",
    "            gaze_now = gaze_results[\"is_looking\"]\n",
    "            if face_now and gaze_now != self.prev_gaze_status:\n",
    "                status = \"Looking\" if gaze_now else \"Away\"\n",
    "                self.logger.log_event(f\"Gaze changed: {status}, Confidence: {gaze_results['smoothed_confidence']:.2f}\")\n",
    "                self.prev_gaze_status = gaze_now\n",
    "\n",
    "            od_now = self.object_detector.is_active\n",
    "            if od_now != self.prev_od_status:\n",
    "                if od_now:\n",
    "                    self.logger.log_event(\"Sustained away gaze detected, activating object detection\")\n",
    "                else:\n",
    "                    self.logger.log_event(\"Object detector deactivated (Gaze back on screen)\")\n",
    "                self.prev_od_status = od_now\n",
    "\n",
    "            if od_now:\n",
    "                for obj in important_objects:\n",
    "                    self.logger.log_event(f\"Detected object: {obj['class_name']} ({obj['confidence']:.2f})\")\n",
    "            \n",
    "            # Update display frame\n",
    "            if self.show_video_feed:\n",
    "                display_frame = gaze_results[\"annotated_frame\"]\n",
    "                \n",
    "                # Add sustained away timer\n",
    "                timer_text = f\"Away Timer: {self.away_duration:.1f}/{self.sustained_away_threshold:.1f}s\"\n",
    "                timer_color = (0, 0, 255) if self.away_duration > 0 else (200, 200, 200)\n",
    "                cv2.putText(display_frame, timer_text, (10, 90), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, timer_color, 2)\n",
    "                \n",
    "                # Add object detection status\n",
    "                od_status = f\"ObjectDetector: {'ACTIVE' if self.object_detector.is_active else 'INACTIVE'}\"\n",
    "                od_color = (0, 255, 0) if self.object_detector.is_active else (100, 100, 100)\n",
    "                cv2.putText(display_frame, od_status, (10, display_frame.shape[0] - 20), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, od_color, 2)\n",
    "                \n",
    "                # Draw object detections if active\n",
    "                for obj in important_objects:\n",
    "                    x1, y1, x2, y2 = obj['box']\n",
    "                    label = f\"{obj['class_name']} {obj['confidence']:.2f}\"\n",
    "                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(display_frame, label, (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                \n",
    "                cv2.imshow(\"Gaze-Controlled Object Detection\", display_frame)\n",
    "\n",
    "            # Check for exit key\n",
    "            key = cv2.waitKey(5) & 0xFF\n",
    "            if key == 27:\n",
    "                print(\"Escape key pressed. Stopping monitoring.\")\n",
    "                self.running = False\n",
    "        \n",
    "        self.stop_monitoring()\n",
    "\n",
    "    def stop_monitoring(self):\n",
    "        print(\"Stopping monitoring and releasing resources...\")\n",
    "        self.running = False\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "            self.cap = None\n",
    "        if self.gaze_detector is not None:\n",
    "            self.gaze_detector.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Monitoring stopped and resources released.\")\n",
    "\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dummy gaze model if needed\n",
    "    if not tf.io.gfile.exists(GAZE_MODEL_PATH):\n",
    "        print(f\"Warning: Model file '{GAZE_MODEL_PATH}' not found. Creating a dummy model for testing.\")\n",
    "        input_left = tf.keras.Input(shape=(64, 64, 3), name=\"left_eye_input\")\n",
    "        input_right = tf.keras.Input(shape=(64, 64, 3), name=\"right_eye_input\")\n",
    "        merged = tf.keras.layers.Concatenate()([tf.keras.layers.Flatten()(input_left), \n",
    "                                                tf.keras.layers.Flatten()(input_right)])\n",
    "        dense1 = tf.keras.layers.Dense(16, activation='relu')(merged)\n",
    "        output = tf.keras.layers.Dense(2, activation='softmax')(dense1)\n",
    "        dummy_keras_model = tf.keras.Model(inputs=[input_left, input_right], outputs=output)\n",
    "        dummy_keras_model.save(GAZE_MODEL_PATH)\n",
    "        print(f\"Dummy model saved to '{GAZE_MODEL_PATH}'. Please replace with your actual model.\")\n",
    "\n",
    "    # Initialize and run the LogicModule\n",
    "    logic_module = LogicModule(\n",
    "        gaze_model_path=GAZE_MODEL_PATH,\n",
    "        sustained_away_threshold=2.0,  # 2 seconds of sustained away gaze\n",
    "        show_video_feed=True\n",
    "    )\n",
    "    try:\n",
    "        logic_module.start_monitoring()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Program interrupted by user (Ctrl+C).\")\n",
    "    finally:\n",
    "        logic_module.stop_monitoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20318fc6-1c31-42d5-85e6-6d8e67c3d026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bills (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
